{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import argparse\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# from model import Model\n",
    "# from dataset import Dataset\n",
    "\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#   device = torch.device('cuda:0') \n",
    "# #   torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "#   torch.backends.cudnn.benchmark = True\n",
    "# else:\n",
    "#    device = torch.device('cpu')\n",
    "# #    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "# print('Using device:', device)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input-Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"data/\"\n",
    "\n",
    "OUTPUT_FOLDER = \"out/\"\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.mkdir(OUTPUT_FOLDER)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV_PATH = INPUT_FOLDER+\"gender_encrypted_385500.csv\"\n",
    "TRAIN_CSV_PATH = INPUT_FOLDER+\"gender_encrypted_385500_train.csv\"\n",
    "VALID_CSV_PATH = INPUT_FOLDER+\"gender_encrypted_385500_valid.csv\"\n",
    "COL_NAME = \"encrypted\" # \"tokenized\", \"encrypted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_FOLDER = COL_NAME\n",
    "MODEL_FOLDER = \"encrypted_Sentence_Single\"\n",
    "\n",
    "MODEL_OUTPUT_FOLDER = OUTPUT_FOLDER+MODEL_FOLDER+\"/\"\n",
    "if not os.path.exists(MODEL_OUTPUT_FOLDER):\n",
    "    os.mkdir(MODEL_OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEREOSET_FOLDER = INPUT_FOLDER+\"Stereoset_Gender_Data/\"\n",
    "EVALUATION_SENTENCES_PATH = STEREOSET_FOLDER+\"sentences_{}.txt\".format(COL_NAME)\n",
    "EVALUATION_IDS_PATH = STEREOSET_FOLDER+\"ids.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_DATA_PATH = STEREOSET_FOLDER+'stereoset-dev-gender-intersentence.json'\n",
    "PREDICTION_SCORES_PATH = MODEL_OUTPUT_FOLDER+'stereoset_prediction_scores.json'\n",
    "RESULTS_PATH = MODEL_OUTPUT_FOLDER+\"results_{}.json\".format(COL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train-Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALID_LEN = 10000\n",
    "# data_df = pd.read_csv(INPUT_CSV_PATH)\n",
    "# values = data_df.values\n",
    "# print(len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(datetime.datetime.now())\n",
    "# val_idx = random.randrange(0, len(values)-VALID_LEN)\n",
    "\n",
    "# valid_values = values[val_idx:val_idx+VALID_LEN]\n",
    "# valid_df = pd.DataFrame(valid_values, columns = data_df.columns)\n",
    "# valid_df.to_csv(VALID_CSV_PATH, index=False)\n",
    "# print(len(valid_df))\n",
    "\n",
    "# train_values = np.delete(values, range(val_idx, val_idx+VALID_LEN), axis=0)\n",
    "# train_df = pd.DataFrame(train_values, columns = data_df.columns)\n",
    "# train_df.to_csv(TRAIN_CSV_PATH, index=False)\n",
    "# print(len(train_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-Fo5AYw9niM"
   },
   "outputs": [],
   "source": [
    "class Tokenizer_VocabBuilder:\n",
    "    def __init__(self, input_csv_path: str, data_col_name: str, vocab_size: int = 10000):\n",
    "        self.input_csv_path = input_csv_path\n",
    "        self.data_col_name = data_col_name\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # input\n",
    "        self.input_df = pd.read_csv(self.input_csv_path)\n",
    "        self.input_sentences = self.input_df[data_col_name].values.tolist()\n",
    "        self.num_examples = len(self.input_sentences)\n",
    "\n",
    "        # initialize\n",
    "        # self.tokenized_sentences = []     # list of strings\n",
    "\n",
    "        # self.num_unique_tokens = 0\n",
    "        # self.unique_tokens_df\n",
    "\n",
    "        # self.vocab_list = []              # list of strings\n",
    "        # self.token2idx = {}\n",
    "        # self.idx2token = {}\n",
    "        \n",
    "    \n",
    "    def tokenize_sentence(self, sentence: str): # can modify this function for more advanced tokenization\n",
    "        sentence = str(sentence).strip()\n",
    "        sentence = ' '.join(sentence.split())\n",
    "        \n",
    "        return sentence\n",
    "\n",
    "\n",
    "    def tokenize_dataset(self):                 \n",
    "        self.tokenized_sentences = []       # list of strings\n",
    "        \n",
    "        for sentence in self.input_sentences:\n",
    "            self.tokenized_sentences.append(self.tokenize_sentence(sentence)) \n",
    "\n",
    "        assert self.num_examples == len(self.tokenized_sentences)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def build_vocab(self):\n",
    "        unique_tokens_dict = {}\n",
    "\n",
    "        self.vocab_list = ['<pad>', '<unk>', '<sos>', '<eos>']              # list of strings\n",
    "\n",
    "        for sentence in self.tokenized_sentences:\n",
    "            for token in sentence.split():\n",
    "                try:\n",
    "                    current_count = unique_tokens_dict[token]\n",
    "                    unique_tokens_dict[token] = current_count+1\n",
    "\n",
    "                except KeyError:\n",
    "                    unique_tokens_dict[token] = 1\n",
    "\n",
    "        self.unique_tokens_df = pd.DataFrame(columns=['token', 'count'])\n",
    "        self.unique_tokens_df['token'] = list(unique_tokens_dict.keys())\n",
    "        self.unique_tokens_df['count'] = list(unique_tokens_dict.values())\n",
    "\n",
    "        self.unique_tokens_df.sort_values(by=['count'], axis=0 , ascending=False, inplace=True, ignore_index=True)\n",
    "        self.num_unique_tokens = len(self.unique_tokens_df)\n",
    "\n",
    "        self.vocab_list.extend(self.unique_tokens_df['token'].values.tolist()[0:self.vocab_size-4]) \n",
    "\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = {}\n",
    "\n",
    "        for idx in range(len(self.vocab_list)):\n",
    "            self.token2idx[self.vocab_list[idx]] = idx\n",
    "            self.idx2token[idx] = self.vocab_list[idx]\n",
    "\n",
    "\n",
    "    def encode_sentence(self, sentence: str):\n",
    "        token_idx_list = []\n",
    "\n",
    "        sentence = self.tokenize_sentence(sentence)\n",
    "        for token in sentence.split():\n",
    "            try:\n",
    "                token_idx_list.append(self.token2idx[token])\n",
    "            \n",
    "            except KeyError:\n",
    "                token_idx_list.append(self.token2idx['<unk>'])\n",
    "\n",
    "        return token_idx_list\n",
    "\n",
    "    def decode_sentence(self, token_idx_list: list):\n",
    "        word_list = []\n",
    "        for token_idx in token_idx_list:\n",
    "            try:\n",
    "                word_list.append(self.idx2token[token_idx])\n",
    "            \n",
    "            except KeyError:\n",
    "                word_list.append('<unk>')\n",
    "\n",
    "        sentence = ' '.join(word_list)\n",
    "\n",
    "        return sentence\n",
    "    \n",
    "tokenizer = Tokenizer_VocabBuilder(INPUT_CSV_PATH, COL_NAME, 8000)\n",
    "tokenizer.tokenize_dataset()\n",
    "tokenizer.build_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L1DPVNPk9-lh",
    "outputId": "f2e306de-9285-4102-dcdf-961591410464"
   },
   "source": [
    "# Dataset for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZnZ3OLI-exX"
   },
   "outputs": [],
   "source": [
    "def pad_sequences(x, max_len):\n",
    "    padded = torch.ones((max_len), dtype=torch.long)\n",
    "    if len(x) > max_len: padded[:] = torch.tensor(x[:max_len] , dtype=torch.long)\n",
    "    else: padded[:len(x)] = torch.tensor(x, dtype=torch.long)\n",
    "    return padded\n",
    "\n",
    "\n",
    "class Dataset_Sentence_Concat(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, tokenizer, csv_path):\n",
    "        \n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.csv_path = csv_path\n",
    "        \n",
    "        self.sentences = self.load_sentences()\n",
    "        self.num_sentences = len(self.sentences)\n",
    "        \n",
    "        self.uniq_words = self.tokenizer.vocab_list\n",
    "        #self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = self.tokenizer.token2idx\n",
    "\n",
    "        #self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "        #self.words_indexes = self.tokenizer.encode_sentence(\" \".join(self.words))\n",
    "\n",
    "        \n",
    "    def load_sentences(self):\n",
    "        data_df = pd.read_csv(self.csv_path) # need to input this csv from init to create the test data set\n",
    "        text = data_df[COL_NAME].str.cat(sep=' <eos> ')\n",
    "        return text.split('<eos>') # this may be changed for niloys complex split function\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "#         return len(self.words_indexes)//self.args[\"sequence_length\"] - self.args[\"sequence_length\"]\n",
    "        return self.num_sentences\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tokens = []\n",
    "        id = index\n",
    "        while (len(tokens) <= self.args[\"sequence_length\"]):\n",
    "            tokens.extend(self.tokenizer.encode_sentence(self.sentences[id]))\n",
    "            id+=1\n",
    "            \n",
    "            if id > self.__len__()-1:\n",
    "                id = 0\n",
    "            \n",
    "        tokens = tokens[:self.args[\"sequence_length\"]]\n",
    "\n",
    "        return (\n",
    "            torch.tensor([2]+tokens[:-1]),\n",
    "            torch.tensor(tokens),\n",
    "        )\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class Dataset_Sentence_Single(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, tokenizer, csv_path):\n",
    "        \n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.csv_path = csv_path\n",
    "        \n",
    "        self.sentences = self.load_sentences()\n",
    "        self.num_sentences = len(self.sentences)\n",
    "        \n",
    "        self.uniq_words = self.tokenizer.vocab_list\n",
    "        #self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = self.tokenizer.token2idx\n",
    "\n",
    "        #self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "        #self.words_indexes = self.tokenizer.encode_sentence(\" \".join(self.words))\n",
    "\n",
    "        \n",
    "    def load_sentences(self):\n",
    "        data_df = pd.read_csv(self.csv_path) # need to input this csv from init to create the test data set\n",
    "        text = data_df[COL_NAME].str.cat(sep=' <eos> ')\n",
    "        return text.split('<eos>') # this may be changed for niloys complex split function\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "#         return len(self.words_indexes)//self.args[\"sequence_length\"] - self.args[\"sequence_length\"]\n",
    "        return self.num_sentences\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tokens = []\n",
    "        tokens.extend(self.tokenizer.encode_sentence(self.sentences[index]))\n",
    "                      \n",
    "        pad_encoded = self.word_to_index['<pad>']\n",
    "        while (len(tokens) < self.args[\"sequence_length\"]):\n",
    "            tokens.append(pad_encoded)\n",
    "        \n",
    "        tokens = tokens[0:self.args[\"sequence_length\"]]\n",
    "        return (\n",
    "            torch.tensor([2]+tokens[:-1]),\n",
    "            torch.tensor(tokens),\n",
    "        )\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfa-lHTY_JrM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 512\n",
    "        self.embedding_dim = 300\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.1, #this was 0.1 previously\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        #print(logits.size())\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train, calc and predict funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5YqKrig-Bgbi",
    "outputId": "6eda194e-c231-4a14-851d-e79157854c62"
   },
   "outputs": [],
   "source": [
    "def calc_stereoset(model):\n",
    "    sentence_file = open(EVALUATION_SENTENCES_PATH, 'r')\n",
    "    sentences = sentence_file.readlines()\n",
    "    sentence_file.close()\n",
    "    \n",
    "    id_file = open(EVALUATION_IDS_PATH, 'r')\n",
    "    ids = id_file.readlines()\n",
    "    id_file.close()\n",
    "    \n",
    "    assert len(sentences) == len(ids)\n",
    "    \n",
    "    out_list = []\n",
    "\n",
    "    for idx in range(len(sentences)):\n",
    "        line = sentences[idx].strip()\n",
    "        sample_id = ids[idx].strip()\n",
    "        \n",
    "        #print(line)\n",
    "        #sent = \"<sos> i know you. You are a lair. i know you. You are a lair. i know you. You are a lair\"\n",
    "        sent = line.strip()\n",
    "        tokens = [2]+tokenizer.encode_sentence(sent)\n",
    "        model.eval()\n",
    "        joint_sentence_probability = []\n",
    "        state_h, state_c = model.init_state(len(tokens))\n",
    "        state_h = state_h.to('cuda')\n",
    "        state_c = state_c.to('cuda')\n",
    "        \n",
    "        x = torch.tensor([tokens]).to('cuda')\n",
    "        \n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "        y_pred = y_pred.to('cuda')\n",
    "#         state_h = state_h.to('cuda')\n",
    "#         state_c = state_c.to('cuda')\n",
    "        #print(y_pred.size())\n",
    "        #return\n",
    "        \n",
    "        pred_start_pos = 1\n",
    "        \n",
    "#         words = line.split()\n",
    "#         for word_idx in range(len(words)):\n",
    "#             if words[word_idx].strip() == \".\":\n",
    "#                 pred_start_pos = word_idx+2  # 2 because of the <sos> we added\n",
    "#                 break\n",
    "        \n",
    "        \n",
    "        for i in range(pred_start_pos, len(tokens)):\n",
    "            p = torch.nn.functional.softmax(y_pred[0][i-1], dim=0).detach().cpu().numpy()\n",
    "            joint_sentence_probability.append(p[tokens[i]])\n",
    "\n",
    "            score = np.sum([np.log2(i) for i in joint_sentence_probability]) \n",
    "            score /= len(joint_sentence_probability)\n",
    "            score = np.power(2, score)\n",
    "            \n",
    "            new_dict = {}\n",
    "            new_dict['id'] = sample_id\n",
    "            new_dict['score'] = score\n",
    "            out_list.append(new_dict)\n",
    "    \n",
    "    model.train() \n",
    "    \n",
    "    out_dict = {}\n",
    "    out_dict['intersentence'] = out_list\n",
    "    out_dict['intrasentence'] = []\n",
    "    \n",
    "    with open(PREDICTION_SCORES_PATH, 'w') as outfile:\n",
    "        json.dump(out_dict, outfile, indent = 2)\n",
    "        outfile.close()\n",
    "\n",
    "      \n",
    "    \n",
    "def calc_confidence(model, dataloader):\n",
    "    print(\"Total number of batches = \", len(dataloader))\n",
    "    model.eval()\n",
    "    sm = 0\n",
    "    cnt = 0\n",
    "        \n",
    "    state_h, state_c = model.init_state(args['sequence_length'])\n",
    "    state_h = state_h.to('cuda')\n",
    "    state_c = state_c.to('cuda')\n",
    "    \n",
    "    perp = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            print(datetime.datetime.now(), \":\", 'batch :', batch)\n",
    "            x = x.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            \n",
    "            #print(y_pred.size())\n",
    "            #print(y_pred.transpose(1, 2).size())\n",
    "            #print(y.size())\n",
    "            perp_cal = []\n",
    "            perp_cal.append(nn.CrossEntropyLoss()(y_pred.transpose(1, 2), y).detach().cpu())\n",
    "            perp.append(torch.exp(torch.stack(perp_cal).sum()/len(perp_cal)))\n",
    "            \n",
    "            #y_pred = y_pred.to('cuda')\n",
    "            #state_h = state_h.to('cuda')\n",
    "            #state_c = state_c.to('cuda')\n",
    "\n",
    "            #state_h = state_h.detach()\n",
    "            #state_c = state_c.detach()\n",
    "\n",
    "        #         print(y_pred.shape)\n",
    "\n",
    "#             for idx in range(len(y_pred)):\n",
    "#                 joint_sentence_probability = []\n",
    "#                 for i in range(1, len(x[idx])):\n",
    "#                     p = torch.nn.functional.softmax(y_pred[idx][i-1], dim=0).detach().cpu().numpy()\n",
    "#                     joint_sentence_probability.append(p[x[idx][i]])\n",
    "#                     score = np.sum([np.log2(i) for i in joint_sentence_probability]) \n",
    "#                     score /= len(joint_sentence_probability)\n",
    "#                     score = np.power(2, score)\n",
    "#                     sm+=score\n",
    "#                     cnt+=1\n",
    "                \n",
    "    model.train()\n",
    "    \n",
    "#     confidence_score = sm/cnt\n",
    "    confidence_score = 0\n",
    "    print(\"Confidence Score = \", confidence_score)\n",
    "    \n",
    "    perplexity = float(sum(perp) / len(perp)) \n",
    "    print(\"Perplexity = \",  perplexity)\n",
    "    #print(\"perplexity = \", torch.stack(perp).sum()/cnt)\n",
    "    #print(\" actual perplexity = \", torch.exp(torch.stack(perp).sum()/cnt))\n",
    "    return confidence_score, perplexity\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     confidence_score = sm/cnt\n",
    "    confidence_score = 0\n",
    "    print(\"Confidence Score = \", confidence_score)\n",
    "    \n",
    "    perplexity = float(sum(perp) / len(perp)) \n",
    "    print(\"Perplexity = \",  perplexity)\n",
    "    #print(\"perplexity = \", torch.stack(perp).sum()/cnt)\n",
    "    #print(\" actual perplexity = \", torch.exp(torch.stack(perp).sum()/cnt))\n",
    "    return confidence_score, perplexity\n",
    "\n",
    "\n",
    "def predict(model, text, next_words=100):\n",
    "    words = [2]+tokenizer.encode_sentence(text)\n",
    "    model.eval()\n",
    "    out = words.copy()\n",
    "    with torch.no_grad():\n",
    "        state_h, state_c = model.init_state(len(words))\n",
    "        state_h = state_h.to('cuda')\n",
    "        state_c = state_c.to('cuda')\n",
    "\n",
    "        for i in range(0, next_words):\n",
    "            #x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).to('cuda')\n",
    "            x = torch.tensor([words]).to('cuda')\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "            last_word_logits = y_pred[0][-1]\n",
    "            p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "            word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "            out.append(word_index)\n",
    "    model.train()\n",
    "    print(\"generated sentence = \")\n",
    "    print(tokenizer.decode_sentence(out))\n",
    "\n",
    "def train(model, args, train_dataloader, valid_dataloader):\n",
    "    model.train()\n",
    "\n",
    "    print(\"total number of steps needed for a single epoch = \", len(train_dataloader))\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=2.3102423292463396e-07)\n",
    "    scheduler = StepLR(optimizer, step_size=20, gamma=0.99)\n",
    "    \n",
    "    for epoch in range(LOADED_EPOCH+1, args['max_epochs']):\n",
    "        state_h, state_c = model.init_state(args['sequence_length'])\n",
    "        state_h = state_h.to('cuda')\n",
    "        state_c = state_c.to('cuda')\n",
    "\n",
    "        for batch, (x, y) in enumerate(train_dataloader):\n",
    "            \n",
    "            x = x.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            '''\n",
    "            y_pred = y_pred.to('cuda')\n",
    "            state_h = state_h.to('cuda')\n",
    "            state_c = state_c.to('cuda')\n",
    "            '''\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "            \n",
    "            loss = criterion(y_pred.transpose(1, 2), y).cuda()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            #predict(model, text = \" i know \")\n",
    "            \n",
    "            #valid_score = calc_confidence(model, valid_dataloader)\n",
    "            \n",
    "            print(datetime.datetime.now(), \":\", { 'epoch': epoch, 'batch': batch, 'loss': loss.item(), 'lr':scheduler.get_lr()[0]})\n",
    "          \n",
    "        # end of epoch. generate figure and save model \n",
    "        predict(model, text = \"he knows that \")\n",
    "        print(datetime.datetime.now(), \":\", \"Generating Confidence Scores: \" + str(epoch))\n",
    "        epoch_list.append(epoch)\n",
    "        \n",
    "        train_c, train_p = calc_confidence(model, train_dataloader) # takes 6x time than training\n",
    "        train_conf_list.append(train_c)\n",
    "        train_perp_list.append(train_p)\n",
    "        \n",
    "        valid_c, valid_p = calc_confidence(model, valid_dataloader) \n",
    "        valid_conf_list.append(valid_c)\n",
    "        valid_perp_list.append(valid_p)\n",
    "        \n",
    "        \n",
    "        plt.clf()\n",
    "        plt.plot(epoch_list, train_conf_list, color='red', linestyle='dashed')\n",
    "        plt.plot(epoch_list, valid_conf_list, color='blue', linestyle='solid')\n",
    "\n",
    "        plt.xlabel('Num Epoch')\n",
    "        plt.ylabel('Conf_Score')\n",
    "        plt.title('Conf_Score VS Epoch')\n",
    "        plt.legend(['Train Conf', 'Valid Conf'], loc='upper right')\n",
    "        plt.savefig(MODEL_OUTPUT_FOLDER+\"Conf_Scores.png\" , format='png', dpi=600)\n",
    "        \n",
    "        plt.clf()\n",
    "        plt.plot(epoch_list, train_perp_list, color='red', linestyle='dashed')\n",
    "        plt.plot(epoch_list, valid_perp_list, color='blue', linestyle='solid')\n",
    "\n",
    "        plt.xlabel('Num Epoch')\n",
    "        plt.ylabel('Perp')\n",
    "        plt.title('Perp VS Epoch')\n",
    "        plt.legend(['Train Perp', 'Valid Perp'], loc='upper right')\n",
    "        plt.savefig(MODEL_OUTPUT_FOLDER+\"Perp_Scores.png\" , format='png', dpi=600)\n",
    "        \n",
    "        print(\"FIGURES SAVED: \" + str(epoch))\n",
    "        \n",
    "        \n",
    "        \n",
    "        score_df = pd.DataFrame(columns = ['epoch', 'train_conf', 'train_perp', 'valid_conf', 'valid_perp'])\n",
    "        score_df['epoch'] = epoch_list\n",
    "        score_df['train_conf'] = train_conf_list\n",
    "        score_df['train_perp'] = train_perp_list\n",
    "        score_df['valid_conf'] = valid_conf_list\n",
    "        score_df['valid_perp'] = valid_perp_list\n",
    "        score_df.to_csv(MODEL_OUTPUT_FOLDER+\"Scores.csv\", index = False)\n",
    "        \n",
    "        \n",
    "        torch.save(model, MODEL_OUTPUT_FOLDER + \"model_\"+str(epoch))\n",
    "        print(\"MODEL SAVED: \" + str(epoch))\n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dataset_Merged_Shift(torch.utils.data.Dataset):\n",
    "#     def __init__(self, args, tokenizer, csv_path):\n",
    "        \n",
    "#         self.args = args\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.csv_path = csv_path\n",
    "        \n",
    "#         self.sentences = self.load_sentences()\n",
    "#         self.words = len(self.sentences)-self.args[\"sequence_length\"]\n",
    "        \n",
    "#         self.uniq_words = self.tokenizer.vocab_list\n",
    "#         #self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "#         self.word_to_index = self.tokenizer.token2idx\n",
    "\n",
    "#         #self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "#         #self.words_indexes = self.tokenizer.encode_sentence(\" \".join(self.words))\n",
    "        \n",
    "#     def load_sentences(self):\n",
    "#         data_df = pd.read_csv(self.csv_path) # need to input this csv from init to create the test data set\n",
    "#         text = data_df[COL_NAME].str.cat(sep=' <eos> ')\n",
    "#         return text.split(' ') # this may be changed for niloys complex split function\n",
    "\n",
    "    \n",
    "#     def __len__(self):\n",
    "# #         return len(self.words_indexes)//self.args[\"sequence_length\"] - self.args[\"sequence_length\"]\n",
    "#         return int(self.words)\n",
    "\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         tokens = []\n",
    "#         text = ' '.join(self.sentences[index : index+self.args[\"sequence_length\"]])\n",
    "#         tokens.extend(self.tokenizer.encode_sentence(text))\n",
    "\n",
    "#         return (\n",
    "#             torch.tensor([2]+tokens[:-1]),\n",
    "#             torch.tensor(tokens),\n",
    "#         )\n",
    "    \n",
    "      \n",
    "        \n",
    "# def calc_merged(model, dataloader, batch_start, num_batches):\n",
    "#     print(\"Total number of batches = \", len(dataloader))\n",
    "#     model.eval()\n",
    "#     sm = 0\n",
    "#     cnt = 0\n",
    "        \n",
    "#     state_h, state_c = model.init_state(args['sequence_length'])\n",
    "#     state_h = state_h.to('cuda')\n",
    "#     state_c = state_c.to('cuda')\n",
    "    \n",
    "#     perp = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch, (x, y) in enumerate(dataloader):\n",
    "#             if batch<batch_start :\n",
    "#                 continue\n",
    "#             if batch>batch_start+num_batches:\n",
    "#                 break\n",
    "            \n",
    "#             print(datetime.datetime.now(), \":\", 'batch :', batch)\n",
    "#             x = x.to('cuda')\n",
    "#             y = y.to('cuda')\n",
    "\n",
    "#             y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            \n",
    "#             #print(y_pred.size())\n",
    "#             #print(y_pred.transpose(1, 2).size())\n",
    "#             #print(y.size())\n",
    "#             perp_cal = []\n",
    "#             perp_cal.append(nn.CrossEntropyLoss()(y_pred.transpose(1, 2), y).detach().cpu())\n",
    "#             perp.append(torch.exp(torch.stack(perp_cal).sum()/len(perp_cal)))\n",
    "            \n",
    "#             #y_pred = y_pred.to('cuda')\n",
    "#             #state_h = state_h.to('cuda')\n",
    "#             #state_c = state_c.to('cuda')\n",
    "\n",
    "#             #state_h = state_h.detach()\n",
    "#             #state_c = state_c.detach()\n",
    "\n",
    "#         #         print(y_pred.shape)\n",
    "\n",
    "# #             for idx in range(len(y_pred)):\n",
    "# #                 joint_sentence_probability = []\n",
    "# #                 for i in range(1, len(x[idx])):\n",
    "# #                     p = torch.nn.functional.softmax(y_pred[idx][i-1], dim=0).detach().cpu().numpy()\n",
    "# #                     joint_sentence_probability.append(p[x[idx][i]])\n",
    "# #                     score = np.sum([np.log2(i) for i in joint_sentence_probability]) \n",
    "# #                     score /= len(joint_sentence_probability)\n",
    "# #                     score = np.power(2, score)\n",
    "# #                     sm+=score\n",
    "# #                     cnt+=1\n",
    "                \n",
    "#     model.train()\n",
    "    \n",
    "# #     confidence_score = sm/cnt\n",
    "#     confidence_score = 0\n",
    "#     print(\"Confidence Score = \", confidence_score)\n",
    "    \n",
    "#     perplexity = float(sum(perp) / len(perp)) \n",
    "#     print(\"Perplexity = \",  perplexity)\n",
    "#     #print(\"perplexity = \", torch.stack(perp).sum()/cnt)\n",
    "#     #print(\" actual perplexity = \", torch.exp(torch.stack(perp).sum()/cnt))\n",
    "#     return confidence_score, perplexity        \n",
    "\n",
    "# def train_merged(model, args, train_dataloader, valid_dataloader):\n",
    "#     model.train()\n",
    "\n",
    "#     print(\"total number of steps needed for a single epoch = \", len(train_dataloader))\n",
    "    \n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# #     optimizer = optim.Adam(model.parameters(), lr=8.88262772170544e-07)\n",
    "#     scheduler = StepLR(optimizer, step_size=20, gamma=0.99)\n",
    "    \n",
    "#     NUM_BATCHES = 446\n",
    "#     MAX_EP = 30\n",
    "    \n",
    "#     for epoch in range(LOADED_EPOCH+1, MAX_EP):\n",
    "#         state_h, state_c = model.init_state(args['sequence_length'])\n",
    "#         state_h = state_h.to('cuda')\n",
    "#         state_c = state_c.to('cuda')\n",
    "\n",
    "#         for batch, (x, y) in enumerate(train_dataloader):\n",
    "#             if batch < epoch*NUM_BATCHES:\n",
    "#                 continue\n",
    "#             if batch > (epoch+1)*NUM_BATCHES:\n",
    "#                 break\n",
    "            \n",
    "#             x = x.to('cuda')\n",
    "#             y = y.to('cuda')\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "#             '''\n",
    "#             y_pred = y_pred.to('cuda')\n",
    "#             state_h = state_h.to('cuda')\n",
    "#             state_c = state_c.to('cuda')\n",
    "#             '''\n",
    "#             state_h = state_h.detach()\n",
    "#             state_c = state_c.detach()\n",
    "            \n",
    "#             loss = criterion(y_pred.transpose(1, 2), y).cuda()\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "            \n",
    "# #             del x\n",
    "# #             del y\n",
    "# #             del y_pred\n",
    "# #             gc.collect()\n",
    "            \n",
    "#             #predict(model, text = \" i know \")\n",
    "            \n",
    "#             #valid_score = calc_confidence(model, valid_dataloader)\n",
    "            \n",
    "#             print(datetime.datetime.now(), \":\", { 'epoch': epoch, 'batch': batch, 'loss': loss.item(), 'lr':scheduler.get_lr()[0]})\n",
    "          \n",
    "#         # end of epoch. generate figure and save model \n",
    "#         predict(model, text = \"he knows that \")\n",
    "#         print(datetime.datetime.now(), \":\", \"Generating Confidence Scores: \" + str(epoch))\n",
    "#         epoch_list.append(epoch)\n",
    "        \n",
    "#         train_c, train_p = calc_merged(model, train_dataloader, epoch*NUM_BATCHES, NUM_BATCHES) # takes 6x time than training\n",
    "#         train_conf_list.append(train_c)\n",
    "#         train_perp_list.append(train_p)\n",
    "        \n",
    "#         valid_c, valid_p = calc_confidence(model, valid_dataloader) \n",
    "#         valid_conf_list.append(valid_c)\n",
    "#         valid_perp_list.append(valid_p)\n",
    "        \n",
    "        \n",
    "#         plt.clf()\n",
    "#         plt.plot(epoch_list, train_conf_list, color='red', linestyle='dashed')\n",
    "#         plt.plot(epoch_list, valid_conf_list, color='blue', linestyle='solid')\n",
    "\n",
    "#         plt.xlabel('Num Epoch')\n",
    "#         plt.ylabel('Conf_Score')\n",
    "#         plt.title('Conf_Score VS Epoch')\n",
    "#         plt.legend(['Train Conf', 'Valid Conf'], loc='upper right')\n",
    "#         plt.savefig(MODEL_OUTPUT_FOLDER+\"Conf_Scores.png\" , format='png', dpi=600)\n",
    "        \n",
    "#         plt.clf()\n",
    "#         plt.plot(epoch_list, train_perp_list, color='red', linestyle='dashed')\n",
    "#         plt.plot(epoch_list, valid_perp_list, color='blue', linestyle='solid')\n",
    "\n",
    "#         plt.xlabel('Num Epoch')\n",
    "#         plt.ylabel('Perp')\n",
    "#         plt.title('Perp VS Epoch')\n",
    "#         plt.legend(['Train Perp', 'Valid Perp'], loc='upper right')\n",
    "#         plt.savefig(MODEL_OUTPUT_FOLDER+\"Perp_Scores.png\" , format='png', dpi=600)\n",
    "        \n",
    "#         print(\"FIGURES SAVED: \" + str(epoch))\n",
    "        \n",
    "        \n",
    "        \n",
    "#         score_df = pd.DataFrame(columns = ['epoch', 'train_conf', 'train_perp', 'valid_conf', 'valid_perp'])\n",
    "#         score_df['epoch'] = epoch_list\n",
    "#         score_df['train_conf'] = train_conf_list\n",
    "#         score_df['train_perp'] = train_perp_list\n",
    "#         score_df['valid_conf'] = valid_conf_list\n",
    "#         score_df['valid_perp'] = valid_perp_list\n",
    "#         score_df.to_csv(MODEL_OUTPUT_FOLDER+\"Scores.csv\", index = False)\n",
    "        \n",
    "        \n",
    "#         torch.save(model, MODEL_OUTPUT_FOLDER + \"model_\"+str(epoch))\n",
    "#         print(\"MODEL SAVED: \" + str(epoch))\n",
    "        \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"sequence_length\"] = 45 # 40->tokenized, 45->encrypted\n",
    "args[\"max_epochs\"] = 50\n",
    "args[\"batch_size\"] = 700\n",
    "\n",
    "'''\n",
    "parser.add_argument('--max-epochs', type=int, default=10)\n",
    "parser.add_argument('--batch-size', type=int, default=256)\n",
    "parser.add_argument('--sequence-length', type=int, default=4)\n",
    "'''\n",
    "\n",
    "train_dataset = Dataset_Sentence_Single(args, tokenizer, TRAIN_CSV_PATH)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args['batch_size'], num_workers = 6, shuffle=True)\n",
    "\n",
    "valid_dataset = Dataset_Sentence_Single(args, tokenizer, VALID_CSV_PATH)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=args['batch_size'], num_workers = 6, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Model\n",
    "\n",
    "model = Model(train_dataset)\n",
    "model = model.to('cuda')\n",
    "\n",
    "LOADED_EPOCH = -1\n",
    "epoch_list = []\n",
    "train_conf_list = []\n",
    "train_perp_list = []\n",
    "valid_conf_list = []\n",
    "valid_perp_list = []\n",
    "\n",
    "train(model, args, train_dataloader, valid_dataloader) \n",
    "# train_merged(model, args, train_dataloader, valid_dataloader) \n",
    "#print(predict(dataset, model, text='Knock knock. Whos there?'))\n",
    "\n",
    "'''\n",
    "Confidence Score =  0.0071846294500849\n",
    "perplexity =  tensor(268.4095)\n",
    "FIGURE SAVED: 24\n",
    "MODEL SAVED: 24\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Preload Model\n",
    "# LOADED_EPOCH = 26\n",
    "\n",
    "# MODEL_NAME = MODEL_OUTPUT_FOLDER + \"model_\"+str(LOADED_EPOCH)\n",
    "# model = torch.load(MODEL_NAME)\n",
    "# model = model.to('cuda')\n",
    "\n",
    "# score_df = pd.read_csv(MODEL_OUTPUT_FOLDER+\"Scores.csv\")\n",
    "# epoch_list = score_df['epoch'].values.tolist()\n",
    "# train_conf_list = score_df['train_conf'].values.tolist()\n",
    "# train_perp_list = score_df['train_perp'].values.tolist()\n",
    "# valid_conf_list = score_df['valid_conf'].values.tolist()\n",
    "# valid_perp_list = score_df['valid_perp'].values.tolist()\n",
    "\n",
    "# train(model, args, train_dataloader, valid_dataloader)\n",
    "# # train_merged(model, args, train_dataloader, valid_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Model from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate prediction scores for Stereoset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = MODEL_OUTPUT_FOLDER + \"model_0\"\n",
    "model = torch.load(MODEL_NAME)\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_stereoset(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on StereoSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/moinnadeemf = open('Stereoset_gender_intersentece_dev_data/sentences.txt', 'r')\n",
    "# for line in f.readlines():\n",
    "#   print(line)/StereoSet.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python StereoSet/code/evaluation.py --gold-file $DEV_DATA_PATH --predictions-file $PREDICTION_SCORES_PATH\n",
    "!cp \"results.json\" $RESULTS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"She was confident in herself, but afraid to face the boys club in the industry. She started\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lz1u2FdkNgGp"
   },
   "outputs": [],
   "source": [
    "#??StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nZFJKRzOBoyv",
    "outputId": "14a806e9-6d35-4204-ffd4-6ea5a8f5a5b4"
   },
   "outputs": [],
   "source": [
    "# sent = \"<sos> i know you. You are a lair. i know you. You are a lair. i know you. You are a lair\"\n",
    "# tokens = tokenizer.encode_sentence(sent)\n",
    "# hypothesis = [0]*len(tokens)\n",
    "# hypothesis[-1] = 2 # <sos> token\n",
    "# '''\n",
    "# while len(tokens)<args[\"sequence_length\"]:\n",
    "#   tokens = [0]+tokens # add initial pads\n",
    "# tokens = [-args[\"sequence_length\"]:]\n",
    "# '''\n",
    "\n",
    "# def calc():\n",
    "#     model.eval()\n",
    "#     joint_sentence_probability = []\n",
    "#     state_h, state_c = model.init_state(len(tokens))\n",
    "#     state_h = state_h.to('cuda')\n",
    "#     state_c = state_c.to('cuda')\n",
    "    \n",
    "#     x = torch.tensor([tokens]).to('cuda')\n",
    "    \n",
    "#     y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "#     y_pred = y_pred.to('cuda')\n",
    "#     state_h = state_h.to('cuda')\n",
    "#     state_c = state_c.to('cuda')\n",
    "#     #print(y_pred.size())\n",
    "#     #return\n",
    "    \n",
    "#     for i in range(1, len(tokens)):\n",
    "#       p = torch.nn.functional.softmax(y_pred[0][i-1], dim=0).cpu().detach().numpy()\n",
    "#       joint_sentence_probability.append(p[tokens[i]])\n",
    "#     score = np.sum([np.log2(i) for i in joint_sentence_probability]) \n",
    "#     score /= len(joint_sentence_probability)\n",
    "#     score = np.power(2, score)\n",
    "#     print(score)\n",
    "# calc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Db2GbcvGN4w1",
    "outputId": "5d796110-2f04-4254-cf40-aa2ffb95e812"
   },
   "outputs": [],
   "source": [
    "# sent = \"<sos> i know you. You are a lair. i know you. You are a lair. i know you. You are a lair\"\n",
    "# tokens = tokenizer.encode_sentence(sent)\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "45WuUks5HikY",
    "outputId": "52256d54-fdb3-436a-efd5-227aa3bf15a2"
   },
   "outputs": [],
   "source": [
    "# !ls Stereoset_gender_intersentece_dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "j-HzR1rvXTfd",
    "outputId": "ba7830bc-66d1-4c93-fb88-7b5e8758ec15"
   },
   "outputs": [],
   "source": [
    "# f = open('Stereoset_gender_intersentece_dev_data/sentences.txt', 'r')\n",
    "# for line in f.readlines():\n",
    "#   print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQQbKOxXXvr4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenized_sentences[43536] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for line in tokenizer.tokenized_sentences:\n",
    "#     c = 0\n",
    "#     line  = line.split()\n",
    "#     for token in line:\n",
    "#         if token.strip() == \".\":\n",
    "#             c += 1\n",
    "#     if c > 1:\n",
    "#         count += 1\n",
    "\n",
    "        \n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.tensor(124.341)\n",
    "# print(a)\n",
    "# print(float(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of RNN_LM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
