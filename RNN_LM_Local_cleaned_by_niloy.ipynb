{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import argparse\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# from model import Model\n",
    "# from dataset import Dataset\n",
    "\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#   device = torch.device('cuda:0') \n",
    "# #   torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "#   torch.backends.cudnn.benchmark = True\n",
    "# else:\n",
    "#    device = torch.device('cpu')\n",
    "# #    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "# print('Using device:', device)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input-Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"data/\"\n",
    "\n",
    "OUTPUT_FOLDER = \"out/\"\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.mkdir(OUTPUT_FOLDER)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV_PATH = INPUT_FOLDER+\"gender_encrypted_385500.csv\"\n",
    "TRAIN_CSV_PATH = INPUT_FOLDER+\"gender_encrypted_385500_train.csv\"\n",
    "VALID_CSV_PATH = INPUT_FOLDER+\"gender_encrypted_385500_valid.csv\"\n",
    "COL_NAME = \"encrypted\" # \"tokenized\", \"encrypted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_FOLDER = COL_NAME\n",
    "MODEL_FOLDER = \"encrypted_Sentence_Single\"\n",
    "\n",
    "MODEL_OUTPUT_FOLDER = OUTPUT_FOLDER+MODEL_FOLDER+\"/\"\n",
    "if not os.path.exists(MODEL_OUTPUT_FOLDER):\n",
    "    os.mkdir(MODEL_OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEREOSET_FOLDER = INPUT_FOLDER+\"Stereoset_Gender_Data/\"\n",
    "EVALUATION_SENTENCES_PATH = STEREOSET_FOLDER+\"sentences_{}.txt\".format(COL_NAME)\n",
    "EVALUATION_IDS_PATH = STEREOSET_FOLDER+\"ids.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_DATA_PATH = STEREOSET_FOLDER+'stereoset-dev-gender-intersentence.json'\n",
    "PREDICTION_SCORES_PATH = MODEL_OUTPUT_FOLDER+'stereoset_prediction_scores.json'\n",
    "RESULTS_PATH = MODEL_OUTPUT_FOLDER+\"results_{}.json\".format(COL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train-Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALID_LEN = 10000\n",
    "# data_df = pd.read_csv(INPUT_CSV_PATH)\n",
    "# values = data_df.values\n",
    "# print(len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(datetime.datetime.now())\n",
    "# val_idx = random.randrange(0, len(values)-VALID_LEN)\n",
    "\n",
    "# valid_values = values[val_idx:val_idx+VALID_LEN]\n",
    "# valid_df = pd.DataFrame(valid_values, columns = data_df.columns)\n",
    "# valid_df.to_csv(VALID_CSV_PATH, index=False)\n",
    "# print(len(valid_df))\n",
    "\n",
    "# train_values = np.delete(values, range(val_idx, val_idx+VALID_LEN), axis=0)\n",
    "# train_df = pd.DataFrame(train_values, columns = data_df.columns)\n",
    "# train_df.to_csv(TRAIN_CSV_PATH, index=False)\n",
    "# print(len(train_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-Fo5AYw9niM"
   },
   "outputs": [],
   "source": [
    "class Tokenizer_VocabBuilder:\n",
    "    def __init__(self, input_csv_path: str, data_col_name: str, vocab_size: int = 10000):\n",
    "        self.input_csv_path = input_csv_path\n",
    "        self.data_col_name = data_col_name\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # input\n",
    "        self.input_df = pd.read_csv(self.input_csv_path)\n",
    "        self.input_sentences = self.input_df[data_col_name].values.tolist()\n",
    "        self.num_examples = len(self.input_sentences)\n",
    "\n",
    "        # initialize\n",
    "        # self.tokenized_sentences = []     # list of strings\n",
    "\n",
    "        # self.num_unique_tokens = 0\n",
    "        # self.unique_tokens_df\n",
    "\n",
    "        # self.vocab_list = []              # list of strings\n",
    "        # self.token2idx = {}\n",
    "        # self.idx2token = {}\n",
    "        \n",
    "    \n",
    "    def tokenize_sentence(self, sentence: str): # can modify this function for more advanced tokenization\n",
    "        sentence = str(sentence).strip()\n",
    "        sentence = ' '.join(sentence.split())\n",
    "        \n",
    "        return sentence\n",
    "\n",
    "\n",
    "    def tokenize_dataset(self):                 \n",
    "        self.tokenized_sentences = []       # list of strings\n",
    "        \n",
    "        for sentence in self.input_sentences:\n",
    "            self.tokenized_sentences.append(self.tokenize_sentence(sentence)) \n",
    "\n",
    "        assert self.num_examples == len(self.tokenized_sentences)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def build_vocab(self):\n",
    "        unique_tokens_dict = {}\n",
    "\n",
    "        self.vocab_list = ['<pad>', '<unk>', '<sos>', '<eos>']              # list of strings\n",
    "\n",
    "        for sentence in self.tokenized_sentences:\n",
    "            for token in sentence.split():\n",
    "                try:\n",
    "                    current_count = unique_tokens_dict[token]\n",
    "                    unique_tokens_dict[token] = current_count+1\n",
    "\n",
    "                except KeyError:\n",
    "                    unique_tokens_dict[token] = 1\n",
    "\n",
    "        self.unique_tokens_df = pd.DataFrame(columns=['token', 'count'])\n",
    "        self.unique_tokens_df['token'] = list(unique_tokens_dict.keys())\n",
    "        self.unique_tokens_df['count'] = list(unique_tokens_dict.values())\n",
    "\n",
    "        self.unique_tokens_df.sort_values(by=['count'], axis=0 , ascending=False, inplace=True, ignore_index=True)\n",
    "        self.num_unique_tokens = len(self.unique_tokens_df)\n",
    "\n",
    "        self.vocab_list.extend(self.unique_tokens_df['token'].values.tolist()[0:self.vocab_size-4]) \n",
    "\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = {}\n",
    "\n",
    "        for idx in range(len(self.vocab_list)):\n",
    "            self.token2idx[self.vocab_list[idx]] = idx\n",
    "            self.idx2token[idx] = self.vocab_list[idx]\n",
    "\n",
    "\n",
    "    def encode_sentence(self, sentence: str):\n",
    "        token_idx_list = []\n",
    "\n",
    "        sentence = self.tokenize_sentence(sentence)\n",
    "        for token in sentence.split():\n",
    "            try:\n",
    "                token_idx_list.append(self.token2idx[token])\n",
    "            \n",
    "            except KeyError:\n",
    "                token_idx_list.append(self.token2idx['<unk>'])\n",
    "\n",
    "        return token_idx_list\n",
    "\n",
    "    def decode_sentence(self, token_idx_list: list):\n",
    "        word_list = []\n",
    "        for token_idx in token_idx_list:\n",
    "            try:\n",
    "                word_list.append(self.idx2token[token_idx])\n",
    "            \n",
    "            except KeyError:\n",
    "                word_list.append('<unk>')\n",
    "\n",
    "        sentence = ' '.join(word_list)\n",
    "\n",
    "        return sentence\n",
    "    \n",
    "tokenizer = Tokenizer_VocabBuilder(INPUT_CSV_PATH, COL_NAME, 8000)\n",
    "tokenizer.tokenize_dataset()\n",
    "tokenizer.build_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L1DPVNPk9-lh",
    "outputId": "f2e306de-9285-4102-dcdf-961591410464"
   },
   "source": [
    "# Dataset for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZnZ3OLI-exX"
   },
   "outputs": [],
   "source": [
    "def pad_sequences(x, max_len):\n",
    "    padded = torch.ones((max_len), dtype=torch.long)\n",
    "    if len(x) > max_len: padded[:] = torch.tensor(x[:max_len] , dtype=torch.long)\n",
    "    else: padded[:len(x)] = torch.tensor(x, dtype=torch.long)\n",
    "    return padded\n",
    "\n",
    "\n",
    "class Dataset_Sentence_Concat(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, tokenizer, csv_path):\n",
    "        \n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.csv_path = csv_path\n",
    "        \n",
    "        self.sentences = self.load_sentences()\n",
    "        self.num_sentences = len(self.sentences)\n",
    "        \n",
    "        self.uniq_words = self.tokenizer.vocab_list\n",
    "        #self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = self.tokenizer.token2idx\n",
    "\n",
    "        #self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "        #self.words_indexes = self.tokenizer.encode_sentence(\" \".join(self.words))\n",
    "\n",
    "        \n",
    "    def load_sentences(self):\n",
    "        data_df = pd.read_csv(self.csv_path) # need to input this csv from init to create the test data set\n",
    "        text = data_df[COL_NAME].str.cat(sep=' <eos> ')\n",
    "        return text.split('<eos>') # this may be changed for niloys complex split function\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "#         return len(self.words_indexes)//self.args[\"sequence_length\"] - self.args[\"sequence_length\"]\n",
    "        return self.num_sentences\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tokens = []\n",
    "        id = index\n",
    "        while (len(tokens) <= self.args[\"sequence_length\"]):\n",
    "            tokens.extend(self.tokenizer.encode_sentence(self.sentences[id]))\n",
    "            id+=1\n",
    "            \n",
    "            if id > self.__len__()-1:\n",
    "                id = 0\n",
    "            \n",
    "        tokens = tokens[:self.args[\"sequence_length\"]]\n",
    "\n",
    "        return (\n",
    "            torch.tensor([2]+tokens[:-1]),\n",
    "            torch.tensor(tokens),\n",
    "        )\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class Dataset_Sentence_Single(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, tokenizer, csv_path):\n",
    "        \n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.csv_path = csv_path\n",
    "        \n",
    "        self.sentences = self.load_sentences()\n",
    "        self.num_sentences = len(self.sentences)\n",
    "        \n",
    "        self.uniq_words = self.tokenizer.vocab_list\n",
    "        #self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = self.tokenizer.token2idx\n",
    "\n",
    "        #self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "        #self.words_indexes = self.tokenizer.encode_sentence(\" \".join(self.words))\n",
    "\n",
    "        \n",
    "    def load_sentences(self):\n",
    "        data_df = pd.read_csv(self.csv_path) # need to input this csv from init to create the test data set\n",
    "        text = data_df[COL_NAME].str.cat(sep=' <eos> ')\n",
    "        return text.split('<eos>') # this may be changed for niloys complex split function\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "#         return len(self.words_indexes)//self.args[\"sequence_length\"] - self.args[\"sequence_length\"]\n",
    "        return self.num_sentences\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tokens = []\n",
    "        tokens.extend(self.tokenizer.encode_sentence(self.sentences[index]))\n",
    "                      \n",
    "        pad_encoded = self.word_to_index['<pad>']\n",
    "        while (len(tokens) < self.args[\"sequence_length\"]):\n",
    "            tokens.append(pad_encoded)\n",
    "        \n",
    "        tokens = tokens[0:self.args[\"sequence_length\"]]\n",
    "        return (\n",
    "            torch.tensor([2]+tokens[:-1]),\n",
    "            torch.tensor(tokens),\n",
    "        )\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfa-lHTY_JrM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 512\n",
    "        self.embedding_dim = 300\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.1, #this was 0.1 previously\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        #print(logits.size())\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train, calc and predict funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5YqKrig-Bgbi",
    "outputId": "6eda194e-c231-4a14-851d-e79157854c62"
   },
   "outputs": [],
   "source": [
    "def calc_stereoset(model):\n",
    "    sentence_file = open(EVALUATION_SENTENCES_PATH, 'r')\n",
    "    sentences = sentence_file.readlines()\n",
    "    sentence_file.close()\n",
    "    \n",
    "    id_file = open(EVALUATION_IDS_PATH, 'r')\n",
    "    ids = id_file.readlines()\n",
    "    id_file.close()\n",
    "    \n",
    "    assert len(sentences) == len(ids)\n",
    "    \n",
    "    out_list = []\n",
    "\n",
    "    for idx in range(len(sentences)):\n",
    "        line = sentences[idx].strip()\n",
    "        sample_id = ids[idx].strip()\n",
    "        \n",
    "        #print(line)\n",
    "        #sent = \"<sos> i know you. You are a lair. i know you. You are a lair. i know you. You are a lair\"\n",
    "        sent = line.strip()\n",
    "        tokens = [2]+tokenizer.encode_sentence(sent)\n",
    "        model.eval()\n",
    "        joint_sentence_probability = []\n",
    "        state_h, state_c = model.init_state(len(tokens))\n",
    "        state_h = state_h.to('cuda')\n",
    "        state_c = state_c.to('cuda')\n",
    "        \n",
    "        x = torch.tensor([tokens]).to('cuda')\n",
    "        \n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "        y_pred = y_pred.to('cuda')\n",
    "#         state_h = state_h.to('cuda')\n",
    "#         state_c = state_c.to('cuda')\n",
    "        #print(y_pred.size())\n",
    "        #return\n",
    "        \n",
    "        pred_start_pos = 1\n",
    "        \n",
    "#         words = line.split()\n",
    "#         for word_idx in range(len(words)):\n",
    "#             if words[word_idx].strip() == \".\":\n",
    "#                 pred_start_pos = word_idx+2  # 2 because of the <sos> we added\n",
    "#                 break\n",
    "        \n",
    "        \n",
    "        for i in range(pred_start_pos, len(tokens)):\n",
    "            p = torch.nn.functional.softmax(y_pred[0][i-1], dim=0).detach().cpu().numpy()\n",
    "            joint_sentence_probability.append(p[tokens[i]])\n",
    "\n",
    "            score = np.sum([np.log2(i) for i in joint_sentence_probability]) \n",
    "            score /= len(joint_sentence_probability)\n",
    "            score = np.power(2, score)\n",
    "            \n",
    "            new_dict = {}\n",
    "            new_dict['id'] = sample_id\n",
    "            new_dict['score'] = score\n",
    "            out_list.append(new_dict)\n",
    "    \n",
    "    model.train() \n",
    "    \n",
    "    out_dict = {}\n",
    "    out_dict['intersentence'] = out_list\n",
    "    out_dict['intrasentence'] = []\n",
    "    \n",
    "    with open(PREDICTION_SCORES_PATH, 'w') as outfile:\n",
    "        json.dump(out_dict, outfile, indent = 2)\n",
    "        outfile.close()\n",
    "\n",
    "      \n",
    "    \n",
    "def calc_confidence(model, dataloader):\n",
    "    print(\"Total number of batches = \", len(dataloader))\n",
    "    model.eval()\n",
    "    sm = 0\n",
    "    cnt = 0\n",
    "        \n",
    "    state_h, state_c = model.init_state(args['sequence_length'])\n",
    "    state_h = state_h.to('cuda')\n",
    "    state_c = state_c.to('cuda')\n",
    "    \n",
    "    perp = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            print(datetime.datetime.now(), \":\", 'batch :', batch)\n",
    "            x = x.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            \n",
    "            #print(y_pred.size())\n",
    "            #print(y_pred.transpose(1, 2).size())\n",
    "            #print(y.size())\n",
    "            perp_cal = []\n",
    "            perp_cal.append(nn.CrossEntropyLoss()(y_pred.transpose(1, 2), y).detach().cpu())\n",
    "            perp.append(torch.exp(torch.stack(perp_cal).sum()/len(perp_cal)))\n",
    "            \n",
    "            #y_pred = y_pred.to('cuda')\n",
    "            #state_h = state_h.to('cuda')\n",
    "            #state_c = state_c.to('cuda')\n",
    "\n",
    "            #state_h = state_h.detach()\n",
    "            #state_c = state_c.detach()\n",
    "\n",
    "        #         print(y_pred.shape)\n",
    "\n",
    "#             for idx in range(len(y_pred)):\n",
    "#                 joint_sentence_probability = []\n",
    "#                 for i in range(1, len(x[idx])):\n",
    "#                     p = torch.nn.functional.softmax(y_pred[idx][i-1], dim=0).detach().cpu().numpy()\n",
    "#                     joint_sentence_probability.append(p[x[idx][i]])\n",
    "#                     score = np.sum([np.log2(i) for i in joint_sentence_probability]) \n",
    "#                     score /= len(joint_sentence_probability)\n",
    "#                     score = np.power(2, score)\n",
    "#                     sm+=score\n",
    "#                     cnt+=1\n",
    "                \n",
    "    model.train()\n",
    "    \n",
    "#     confidence_score = sm/cnt\n",
    "    confidence_score = 0\n",
    "    print(\"Confidence Score = \", confidence_score)\n",
    "    \n",
    "    perplexity = float(sum(perp) / len(perp)) \n",
    "    print(\"Perplexity = \",  perplexity)\n",
    "    #print(\"perplexity = \", torch.stack(perp).sum()/cnt)\n",
    "    #print(\" actual perplexity = \", torch.exp(torch.stack(perp).sum()/cnt))\n",
    "    return confidence_score, perplexity\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     confidence_score = sm/cnt\n",
    "    confidence_score = 0\n",
    "    print(\"Confidence Score = \", confidence_score)\n",
    "    \n",
    "    perplexity = float(sum(perp) / len(perp)) \n",
    "    print(\"Perplexity = \",  perplexity)\n",
    "    #print(\"perplexity = \", torch.stack(perp).sum()/cnt)\n",
    "    #print(\" actual perplexity = \", torch.exp(torch.stack(perp).sum()/cnt))\n",
    "    return confidence_score, perplexity\n",
    "\n",
    "\n",
    "def predict(model, text, next_words=100):\n",
    "    words = [2]+tokenizer.encode_sentence(text)\n",
    "    model.eval()\n",
    "    out = words.copy()\n",
    "    with torch.no_grad():\n",
    "        state_h, state_c = model.init_state(len(words))\n",
    "        state_h = state_h.to('cuda')\n",
    "        state_c = state_c.to('cuda')\n",
    "\n",
    "        for i in range(0, next_words):\n",
    "            #x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).to('cuda')\n",
    "            x = torch.tensor([words]).to('cuda')\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "            last_word_logits = y_pred[0][-1]\n",
    "            p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "            word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "            out.append(word_index)\n",
    "    model.train()\n",
    "    print(\"generated sentence = \")\n",
    "    print(tokenizer.decode_sentence(out))\n",
    "\n",
    "def train(model, args, train_dataloader, valid_dataloader):\n",
    "    model.train()\n",
    "\n",
    "    print(\"total number of steps needed for a single epoch = \", len(train_dataloader))\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=2.3102423292463396e-07)\n",
    "    scheduler = StepLR(optimizer, step_size=20, gamma=0.99)\n",
    "    \n",
    "    for epoch in range(LOADED_EPOCH+1, args['max_epochs']):\n",
    "        state_h, state_c = model.init_state(args['sequence_length'])\n",
    "        state_h = state_h.to('cuda')\n",
    "        state_c = state_c.to('cuda')\n",
    "\n",
    "        for batch, (x, y) in enumerate(train_dataloader):\n",
    "            \n",
    "            x = x.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            '''\n",
    "            y_pred = y_pred.to('cuda')\n",
    "            state_h = state_h.to('cuda')\n",
    "            state_c = state_c.to('cuda')\n",
    "            '''\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "            \n",
    "            loss = criterion(y_pred.transpose(1, 2), y).cuda()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            #predict(model, text = \" i know \")\n",
    "            \n",
    "            #valid_score = calc_confidence(model, valid_dataloader)\n",
    "            \n",
    "            print(datetime.datetime.now(), \":\", { 'epoch': epoch, 'batch': batch, 'loss': loss.item(), 'lr':scheduler.get_lr()[0]})\n",
    "          \n",
    "        # end of epoch. generate figure and save model \n",
    "        predict(model, text = \"he knows that \")\n",
    "        print(datetime.datetime.now(), \":\", \"Generating Confidence Scores: \" + str(epoch))\n",
    "        epoch_list.append(epoch)\n",
    "        \n",
    "        train_c, train_p = calc_confidence(model, train_dataloader) # takes 6x time than training\n",
    "        train_conf_list.append(train_c)\n",
    "        train_perp_list.append(train_p)\n",
    "        \n",
    "        valid_c, valid_p = calc_confidence(model, valid_dataloader) \n",
    "        valid_conf_list.append(valid_c)\n",
    "        valid_perp_list.append(valid_p)\n",
    "        \n",
    "        \n",
    "        plt.clf()\n",
    "        plt.plot(epoch_list, train_conf_list, color='red', linestyle='dashed')\n",
    "        plt.plot(epoch_list, valid_conf_list, color='blue', linestyle='solid')\n",
    "\n",
    "        plt.xlabel('Num Epoch')\n",
    "        plt.ylabel('Conf_Score')\n",
    "        plt.title('Conf_Score VS Epoch')\n",
    "        plt.legend(['Train Conf', 'Valid Conf'], loc='upper right')\n",
    "        plt.savefig(MODEL_OUTPUT_FOLDER+\"Conf_Scores.png\" , format='png', dpi=600)\n",
    "        \n",
    "        plt.clf()\n",
    "        plt.plot(epoch_list, train_perp_list, color='red', linestyle='dashed')\n",
    "        plt.plot(epoch_list, valid_perp_list, color='blue', linestyle='solid')\n",
    "\n",
    "        plt.xlabel('Num Epoch')\n",
    "        plt.ylabel('Perp')\n",
    "        plt.title('Perp VS Epoch')\n",
    "        plt.legend(['Train Perp', 'Valid Perp'], loc='upper right')\n",
    "        plt.savefig(MODEL_OUTPUT_FOLDER+\"Perp_Scores.png\" , format='png', dpi=600)\n",
    "        \n",
    "        print(\"FIGURES SAVED: \" + str(epoch))\n",
    "        \n",
    "        \n",
    "        \n",
    "        score_df = pd.DataFrame(columns = ['epoch', 'train_conf', 'train_perp', 'valid_conf', 'valid_perp'])\n",
    "        score_df['epoch'] = epoch_list\n",
    "        score_df['train_conf'] = train_conf_list\n",
    "        score_df['train_perp'] = train_perp_list\n",
    "        score_df['valid_conf'] = valid_conf_list\n",
    "        score_df['valid_perp'] = valid_perp_list\n",
    "        score_df.to_csv(MODEL_OUTPUT_FOLDER+\"Scores.csv\", index = False)\n",
    "        \n",
    "        \n",
    "        torch.save(model, MODEL_OUTPUT_FOLDER + \"model_\"+str(epoch))\n",
    "        print(\"MODEL SAVED: \" + str(epoch))\n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dataset_Merged_Shift(torch.utils.data.Dataset):\n",
    "#     def __init__(self, args, tokenizer, csv_path):\n",
    "        \n",
    "#         self.args = args\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.csv_path = csv_path\n",
    "        \n",
    "#         self.sentences = self.load_sentences()\n",
    "#         self.words = len(self.sentences)-self.args[\"sequence_length\"]\n",
    "        \n",
    "#         self.uniq_words = self.tokenizer.vocab_list\n",
    "#         #self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "#         self.word_to_index = self.tokenizer.token2idx\n",
    "\n",
    "#         #self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "#         #self.words_indexes = self.tokenizer.encode_sentence(\" \".join(self.words))\n",
    "        \n",
    "#     def load_sentences(self):\n",
    "#         data_df = pd.read_csv(self.csv_path) # need to input this csv from init to create the test data set\n",
    "#         text = data_df[COL_NAME].str.cat(sep=' <eos> ')\n",
    "#         return text.split(' ') # this may be changed for niloys complex split function\n",
    "\n",
    "    \n",
    "#     def __len__(self):\n",
    "# #         return len(self.words_indexes)//self.args[\"sequence_length\"] - self.args[\"sequence_length\"]\n",
    "#         return int(self.words)\n",
    "\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         tokens = []\n",
    "#         text = ' '.join(self.sentences[index : index+self.args[\"sequence_length\"]])\n",
    "#         tokens.extend(self.tokenizer.encode_sentence(text))\n",
    "\n",
    "#         return (\n",
    "#             torch.tensor([2]+tokens[:-1]),\n",
    "#             torch.tensor(tokens),\n",
    "#         )\n",
    "    \n",
    "      \n",
    "        \n",
    "# def calc_merged(model, dataloader, batch_start, num_batches):\n",
    "#     print(\"Total number of batches = \", len(dataloader))\n",
    "#     model.eval()\n",
    "#     sm = 0\n",
    "#     cnt = 0\n",
    "        \n",
    "#     state_h, state_c = model.init_state(args['sequence_length'])\n",
    "#     state_h = state_h.to('cuda')\n",
    "#     state_c = state_c.to('cuda')\n",
    "    \n",
    "#     perp = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch, (x, y) in enumerate(dataloader):\n",
    "#             if batch<batch_start :\n",
    "#                 continue\n",
    "#             if batch>batch_start+num_batches:\n",
    "#                 break\n",
    "            \n",
    "#             print(datetime.datetime.now(), \":\", 'batch :', batch)\n",
    "#             x = x.to('cuda')\n",
    "#             y = y.to('cuda')\n",
    "\n",
    "#             y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            \n",
    "#             #print(y_pred.size())\n",
    "#             #print(y_pred.transpose(1, 2).size())\n",
    "#             #print(y.size())\n",
    "#             perp_cal = []\n",
    "#             perp_cal.append(nn.CrossEntropyLoss()(y_pred.transpose(1, 2), y).detach().cpu())\n",
    "#             perp.append(torch.exp(torch.stack(perp_cal).sum()/len(perp_cal)))\n",
    "            \n",
    "#             #y_pred = y_pred.to('cuda')\n",
    "#             #state_h = state_h.to('cuda')\n",
    "#             #state_c = state_c.to('cuda')\n",
    "\n",
    "#             #state_h = state_h.detach()\n",
    "#             #state_c = state_c.detach()\n",
    "\n",
    "#         #         print(y_pred.shape)\n",
    "\n",
    "# #             for idx in range(len(y_pred)):\n",
    "# #                 joint_sentence_probability = []\n",
    "# #                 for i in range(1, len(x[idx])):\n",
    "# #                     p = torch.nn.functional.softmax(y_pred[idx][i-1], dim=0).detach().cpu().numpy()\n",
    "# #                     joint_sentence_probability.append(p[x[idx][i]])\n",
    "# #                     score = np.sum([np.log2(i) for i in joint_sentence_probability]) \n",
    "# #                     score /= len(joint_sentence_probability)\n",
    "# #                     score = np.power(2, score)\n",
    "# #                     sm+=score\n",
    "# #                     cnt+=1\n",
    "                \n",
    "#     model.train()\n",
    "    \n",
    "# #     confidence_score = sm/cnt\n",
    "#     confidence_score = 0\n",
    "#     print(\"Confidence Score = \", confidence_score)\n",
    "    \n",
    "#     perplexity = float(sum(perp) / len(perp)) \n",
    "#     print(\"Perplexity = \",  perplexity)\n",
    "#     #print(\"perplexity = \", torch.stack(perp).sum()/cnt)\n",
    "#     #print(\" actual perplexity = \", torch.exp(torch.stack(perp).sum()/cnt))\n",
    "#     return confidence_score, perplexity        \n",
    "\n",
    "# def train_merged(model, args, train_dataloader, valid_dataloader):\n",
    "#     model.train()\n",
    "\n",
    "#     print(\"total number of steps needed for a single epoch = \", len(train_dataloader))\n",
    "    \n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# #     optimizer = optim.Adam(model.parameters(), lr=8.88262772170544e-07)\n",
    "#     scheduler = StepLR(optimizer, step_size=20, gamma=0.99)\n",
    "    \n",
    "#     NUM_BATCHES = 446\n",
    "#     MAX_EP = 30\n",
    "    \n",
    "#     for epoch in range(LOADED_EPOCH+1, MAX_EP):\n",
    "#         state_h, state_c = model.init_state(args['sequence_length'])\n",
    "#         state_h = state_h.to('cuda')\n",
    "#         state_c = state_c.to('cuda')\n",
    "\n",
    "#         for batch, (x, y) in enumerate(train_dataloader):\n",
    "#             if batch < epoch*NUM_BATCHES:\n",
    "#                 continue\n",
    "#             if batch > (epoch+1)*NUM_BATCHES:\n",
    "#                 break\n",
    "            \n",
    "#             x = x.to('cuda')\n",
    "#             y = y.to('cuda')\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "#             '''\n",
    "#             y_pred = y_pred.to('cuda')\n",
    "#             state_h = state_h.to('cuda')\n",
    "#             state_c = state_c.to('cuda')\n",
    "#             '''\n",
    "#             state_h = state_h.detach()\n",
    "#             state_c = state_c.detach()\n",
    "            \n",
    "#             loss = criterion(y_pred.transpose(1, 2), y).cuda()\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "            \n",
    "# #             del x\n",
    "# #             del y\n",
    "# #             del y_pred\n",
    "# #             gc.collect()\n",
    "            \n",
    "#             #predict(model, text = \" i know \")\n",
    "            \n",
    "#             #valid_score = calc_confidence(model, valid_dataloader)\n",
    "            \n",
    "#             print(datetime.datetime.now(), \":\", { 'epoch': epoch, 'batch': batch, 'loss': loss.item(), 'lr':scheduler.get_lr()[0]})\n",
    "          \n",
    "#         # end of epoch. generate figure and save model \n",
    "#         predict(model, text = \"he knows that \")\n",
    "#         print(datetime.datetime.now(), \":\", \"Generating Confidence Scores: \" + str(epoch))\n",
    "#         epoch_list.append(epoch)\n",
    "        \n",
    "#         train_c, train_p = calc_merged(model, train_dataloader, epoch*NUM_BATCHES, NUM_BATCHES) # takes 6x time than training\n",
    "#         train_conf_list.append(train_c)\n",
    "#         train_perp_list.append(train_p)\n",
    "        \n",
    "#         valid_c, valid_p = calc_confidence(model, valid_dataloader) \n",
    "#         valid_conf_list.append(valid_c)\n",
    "#         valid_perp_list.append(valid_p)\n",
    "        \n",
    "        \n",
    "#         plt.clf()\n",
    "#         plt.plot(epoch_list, train_conf_list, color='red', linestyle='dashed')\n",
    "#         plt.plot(epoch_list, valid_conf_list, color='blue', linestyle='solid')\n",
    "\n",
    "#         plt.xlabel('Num Epoch')\n",
    "#         plt.ylabel('Conf_Score')\n",
    "#         plt.title('Conf_Score VS Epoch')\n",
    "#         plt.legend(['Train Conf', 'Valid Conf'], loc='upper right')\n",
    "#         plt.savefig(MODEL_OUTPUT_FOLDER+\"Conf_Scores.png\" , format='png', dpi=600)\n",
    "        \n",
    "#         plt.clf()\n",
    "#         plt.plot(epoch_list, train_perp_list, color='red', linestyle='dashed')\n",
    "#         plt.plot(epoch_list, valid_perp_list, color='blue', linestyle='solid')\n",
    "\n",
    "#         plt.xlabel('Num Epoch')\n",
    "#         plt.ylabel('Perp')\n",
    "#         plt.title('Perp VS Epoch')\n",
    "#         plt.legend(['Train Perp', 'Valid Perp'], loc='upper right')\n",
    "#         plt.savefig(MODEL_OUTPUT_FOLDER+\"Perp_Scores.png\" , format='png', dpi=600)\n",
    "        \n",
    "#         print(\"FIGURES SAVED: \" + str(epoch))\n",
    "        \n",
    "        \n",
    "        \n",
    "#         score_df = pd.DataFrame(columns = ['epoch', 'train_conf', 'train_perp', 'valid_conf', 'valid_perp'])\n",
    "#         score_df['epoch'] = epoch_list\n",
    "#         score_df['train_conf'] = train_conf_list\n",
    "#         score_df['train_perp'] = train_perp_list\n",
    "#         score_df['valid_conf'] = valid_conf_list\n",
    "#         score_df['valid_perp'] = valid_perp_list\n",
    "#         score_df.to_csv(MODEL_OUTPUT_FOLDER+\"Scores.csv\", index = False)\n",
    "        \n",
    "        \n",
    "#         torch.save(model, MODEL_OUTPUT_FOLDER + \"model_\"+str(epoch))\n",
    "#         print(\"MODEL SAVED: \" + str(epoch))\n",
    "        \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"sequence_length\"] = 45 # 40->tokenized, 45->encrypted\n",
    "args[\"max_epochs\"] = 50\n",
    "args[\"batch_size\"] = 700\n",
    "\n",
    "'''\n",
    "parser.add_argument('--max-epochs', type=int, default=10)\n",
    "parser.add_argument('--batch-size', type=int, default=256)\n",
    "parser.add_argument('--sequence-length', type=int, default=4)\n",
    "'''\n",
    "\n",
    "train_dataset = Dataset_Sentence_Single(args, tokenizer, TRAIN_CSV_PATH)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args['batch_size'], num_workers = 6, shuffle=True)\n",
    "\n",
    "valid_dataset = Dataset_Sentence_Single(args, tokenizer, VALID_CSV_PATH)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=args['batch_size'], num_workers = 6, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of steps needed for a single epoch =  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishtiaq/anaconda3/envs/torch/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:350: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-03 22:04:41.062864 : {'epoch': 0, 'batch': 0, 'loss': 8.989461898803711, 'lr': 0.0001}\n",
      "2020-09-03 22:04:41.533777 : {'epoch': 0, 'batch': 1, 'loss': 8.97198486328125, 'lr': 0.0001}\n",
      "2020-09-03 22:04:41.981863 : {'epoch': 0, 'batch': 2, 'loss': 8.953956604003906, 'lr': 0.0001}\n",
      "2020-09-03 22:04:42.498417 : {'epoch': 0, 'batch': 3, 'loss': 8.932896614074707, 'lr': 0.0001}\n",
      "2020-09-03 22:04:42.936253 : {'epoch': 0, 'batch': 4, 'loss': 8.910138130187988, 'lr': 0.0001}\n",
      "2020-09-03 22:04:43.405115 : {'epoch': 0, 'batch': 5, 'loss': 8.88172435760498, 'lr': 0.0001}\n",
      "2020-09-03 22:04:43.837695 : {'epoch': 0, 'batch': 6, 'loss': 8.846268653869629, 'lr': 0.0001}\n",
      "2020-09-03 22:04:44.274451 : {'epoch': 0, 'batch': 7, 'loss': 8.812833786010742, 'lr': 0.0001}\n",
      "2020-09-03 22:04:44.708502 : {'epoch': 0, 'batch': 8, 'loss': 8.76875114440918, 'lr': 0.0001}\n",
      "2020-09-03 22:04:45.143747 : {'epoch': 0, 'batch': 9, 'loss': 8.692233085632324, 'lr': 0.0001}\n",
      "2020-09-03 22:04:45.579467 : {'epoch': 0, 'batch': 10, 'loss': 8.610390663146973, 'lr': 0.0001}\n",
      "2020-09-03 22:04:46.022289 : {'epoch': 0, 'batch': 11, 'loss': 8.502578735351562, 'lr': 0.0001}\n",
      "2020-09-03 22:04:46.457669 : {'epoch': 0, 'batch': 12, 'loss': 8.359817504882812, 'lr': 0.0001}\n",
      "2020-09-03 22:04:46.909031 : {'epoch': 0, 'batch': 13, 'loss': 8.221287727355957, 'lr': 0.0001}\n",
      "2020-09-03 22:04:47.345471 : {'epoch': 0, 'batch': 14, 'loss': 8.020636558532715, 'lr': 0.0001}\n",
      "2020-09-03 22:04:47.808202 : {'epoch': 0, 'batch': 15, 'loss': 7.7999749183654785, 'lr': 0.0001}\n",
      "2020-09-03 22:04:48.269375 : {'epoch': 0, 'batch': 16, 'loss': 7.557467937469482, 'lr': 0.0001}\n",
      "2020-09-03 22:04:48.706264 : {'epoch': 0, 'batch': 17, 'loss': 7.257840156555176, 'lr': 0.0001}\n",
      "2020-09-03 22:04:49.147112 : {'epoch': 0, 'batch': 18, 'loss': 7.0420331954956055, 'lr': 0.0001}\n",
      "2020-09-03 22:04:49.585397 : {'epoch': 0, 'batch': 19, 'loss': 6.856509208679199, 'lr': 9.801e-05}\n",
      "2020-09-03 22:04:50.019896 : {'epoch': 0, 'batch': 20, 'loss': 6.597589015960693, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:50.456516 : {'epoch': 0, 'batch': 21, 'loss': 6.297929286956787, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:50.891157 : {'epoch': 0, 'batch': 22, 'loss': 6.217609882354736, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:51.338660 : {'epoch': 0, 'batch': 23, 'loss': 5.975264072418213, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:51.778024 : {'epoch': 0, 'batch': 24, 'loss': 5.894038200378418, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:52.218230 : {'epoch': 0, 'batch': 25, 'loss': 5.753343105316162, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:52.653850 : {'epoch': 0, 'batch': 26, 'loss': 5.607159614562988, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:53.093532 : {'epoch': 0, 'batch': 27, 'loss': 5.666029453277588, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:53.543518 : {'epoch': 0, 'batch': 28, 'loss': 5.657221794128418, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:53.981001 : {'epoch': 0, 'batch': 29, 'loss': 5.704567909240723, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:54.418038 : {'epoch': 0, 'batch': 30, 'loss': 5.678365707397461, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:54.854980 : {'epoch': 0, 'batch': 31, 'loss': 5.607789039611816, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:55.292477 : {'epoch': 0, 'batch': 32, 'loss': 5.584941387176514, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:55.730628 : {'epoch': 0, 'batch': 33, 'loss': 5.603867053985596, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:56.166748 : {'epoch': 0, 'batch': 34, 'loss': 5.542221546173096, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:56.601984 : {'epoch': 0, 'batch': 35, 'loss': 5.416550159454346, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:57.039950 : {'epoch': 0, 'batch': 36, 'loss': 5.373952388763428, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:57.477022 : {'epoch': 0, 'batch': 37, 'loss': 5.335473537445068, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:57.913883 : {'epoch': 0, 'batch': 38, 'loss': 5.270089626312256, 'lr': 9.900000000000001e-05}\n",
      "2020-09-03 22:04:58.351736 : {'epoch': 0, 'batch': 39, 'loss': 5.179703712463379, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:04:58.791392 : {'epoch': 0, 'batch': 40, 'loss': 5.24884557723999, 'lr': 9.801e-05}\n",
      "2020-09-03 22:04:59.227748 : {'epoch': 0, 'batch': 41, 'loss': 5.191490173339844, 'lr': 9.801e-05}\n",
      "2020-09-03 22:04:59.664026 : {'epoch': 0, 'batch': 42, 'loss': 5.182953834533691, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:00.100560 : {'epoch': 0, 'batch': 43, 'loss': 5.0834221839904785, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:00.541026 : {'epoch': 0, 'batch': 44, 'loss': 4.991855621337891, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:00.987405 : {'epoch': 0, 'batch': 45, 'loss': 5.09985876083374, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:01.464759 : {'epoch': 0, 'batch': 46, 'loss': 5.016254901885986, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:01.922192 : {'epoch': 0, 'batch': 47, 'loss': 5.079355239868164, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:02.374647 : {'epoch': 0, 'batch': 48, 'loss': 5.028736591339111, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:02.822867 : {'epoch': 0, 'batch': 49, 'loss': 4.946625709533691, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:03.257018 : {'epoch': 0, 'batch': 50, 'loss': 4.871941566467285, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:03.720279 : {'epoch': 0, 'batch': 51, 'loss': 4.867410659790039, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:04.177909 : {'epoch': 0, 'batch': 52, 'loss': 4.879698276519775, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:04.643171 : {'epoch': 0, 'batch': 53, 'loss': 4.870967864990234, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:05.113801 : {'epoch': 0, 'batch': 54, 'loss': 4.8737664222717285, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:05.554743 : {'epoch': 0, 'batch': 55, 'loss': 4.799845218658447, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:05.995404 : {'epoch': 0, 'batch': 56, 'loss': 4.742369651794434, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:06.429932 : {'epoch': 0, 'batch': 57, 'loss': 4.792842388153076, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:06.889316 : {'epoch': 0, 'batch': 58, 'loss': 4.7463812828063965, 'lr': 9.801e-05}\n",
      "2020-09-03 22:05:07.339614 : {'epoch': 0, 'batch': 59, 'loss': 4.616366863250732, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:07.775347 : {'epoch': 0, 'batch': 60, 'loss': 4.74498176574707, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:08.213812 : {'epoch': 0, 'batch': 61, 'loss': 4.711392402648926, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:08.648480 : {'epoch': 0, 'batch': 62, 'loss': 4.7228851318359375, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:09.095144 : {'epoch': 0, 'batch': 63, 'loss': 4.588165283203125, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:09.528176 : {'epoch': 0, 'batch': 64, 'loss': 4.659173965454102, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:09.968941 : {'epoch': 0, 'batch': 65, 'loss': 4.539244651794434, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:10.412002 : {'epoch': 0, 'batch': 66, 'loss': 4.595280647277832, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:10.847629 : {'epoch': 0, 'batch': 67, 'loss': 4.553553104400635, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:11.293685 : {'epoch': 0, 'batch': 68, 'loss': 4.562472820281982, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:11.755539 : {'epoch': 0, 'batch': 69, 'loss': 4.558200836181641, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:12.190915 : {'epoch': 0, 'batch': 70, 'loss': 4.514959812164307, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:12.628649 : {'epoch': 0, 'batch': 71, 'loss': 4.476443767547607, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:13.070956 : {'epoch': 0, 'batch': 72, 'loss': 4.506161212921143, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:13.517776 : {'epoch': 0, 'batch': 73, 'loss': 4.443774223327637, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:13.969790 : {'epoch': 0, 'batch': 74, 'loss': 4.511731147766113, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:14.409379 : {'epoch': 0, 'batch': 75, 'loss': 4.474100112915039, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:14.854932 : {'epoch': 0, 'batch': 76, 'loss': 4.410470962524414, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:15.308830 : {'epoch': 0, 'batch': 77, 'loss': 4.345972061157227, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:15.774909 : {'epoch': 0, 'batch': 78, 'loss': 4.380795478820801, 'lr': 9.70299e-05}\n",
      "2020-09-03 22:05:16.218809 : {'epoch': 0, 'batch': 79, 'loss': 4.312554359436035, 'lr': 9.509900499000001e-05}\n",
      "2020-09-03 22:05:16.669273 : {'epoch': 0, 'batch': 80, 'loss': 4.464076519012451, 'lr': 9.605960100000001e-05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-03 22:05:17.137859 : {'epoch': 0, 'batch': 81, 'loss': 4.445968151092529, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:17.606832 : {'epoch': 0, 'batch': 82, 'loss': 4.42917013168335, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:18.081242 : {'epoch': 0, 'batch': 83, 'loss': 4.352742671966553, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:18.570479 : {'epoch': 0, 'batch': 84, 'loss': 4.418341636657715, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:19.040831 : {'epoch': 0, 'batch': 85, 'loss': 4.332901954650879, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:19.491669 : {'epoch': 0, 'batch': 86, 'loss': 4.310606479644775, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:19.929578 : {'epoch': 0, 'batch': 87, 'loss': 4.366903305053711, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:20.367393 : {'epoch': 0, 'batch': 88, 'loss': 4.34684419631958, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:20.802130 : {'epoch': 0, 'batch': 89, 'loss': 4.336323261260986, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:21.236374 : {'epoch': 0, 'batch': 90, 'loss': 4.285676002502441, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:21.669888 : {'epoch': 0, 'batch': 91, 'loss': 4.367142200469971, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:22.102442 : {'epoch': 0, 'batch': 92, 'loss': 4.3420844078063965, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:22.539475 : {'epoch': 0, 'batch': 93, 'loss': 4.342121124267578, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:22.994669 : {'epoch': 0, 'batch': 94, 'loss': 4.363032817840576, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:23.474814 : {'epoch': 0, 'batch': 95, 'loss': 4.30537748336792, 'lr': 9.605960100000001e-05}\n",
      "2020-09-03 22:05:23.933191 : {'epoch': 0, 'batch': 96, 'loss': 4.286765098571777, 'lr': 9.605960100000001e-05}\n"
     ]
    }
   ],
   "source": [
    "# New Model\n",
    "\n",
    "model = Model(train_dataset)\n",
    "model = model.to('cuda')\n",
    "\n",
    "LOADED_EPOCH = -1\n",
    "epoch_list = []\n",
    "train_conf_list = []\n",
    "train_perp_list = []\n",
    "valid_conf_list = []\n",
    "valid_perp_list = []\n",
    "\n",
    "train(model, args, train_dataloader, valid_dataloader) \n",
    "# train_merged(model, args, train_dataloader, valid_dataloader) \n",
    "#print(predict(dataset, model, text='Knock knock. Whos there?'))\n",
    "\n",
    "'''\n",
    "Confidence Score =  0.0071846294500849\n",
    "perplexity =  tensor(268.4095)\n",
    "FIGURE SAVED: 24\n",
    "MODEL SAVED: 24\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Preload Model\n",
    "# LOADED_EPOCH = 26\n",
    "\n",
    "# MODEL_NAME = MODEL_OUTPUT_FOLDER + \"model_\"+str(LOADED_EPOCH)\n",
    "# model = torch.load(MODEL_NAME)\n",
    "# model = model.to('cuda')\n",
    "\n",
    "# score_df = pd.read_csv(MODEL_OUTPUT_FOLDER+\"Scores.csv\")\n",
    "# epoch_list = score_df['epoch'].values.tolist()\n",
    "# train_conf_list = score_df['train_conf'].values.tolist()\n",
    "# train_perp_list = score_df['train_perp'].values.tolist()\n",
    "# valid_conf_list = score_df['valid_conf'].values.tolist()\n",
    "# valid_perp_list = score_df['valid_perp'].values.tolist()\n",
    "\n",
    "# train(model, args, train_dataloader, valid_dataloader)\n",
    "# # train_merged(model, args, train_dataloader, valid_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Model from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate prediction scores for Stereoset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = MODEL_OUTPUT_FOLDER + \"model_0\"\n",
    "model = torch.load(MODEL_NAME)\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_stereoset(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on StereoSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/moinnadeemf = open('Stereoset_gender_intersentece_dev_data/sentences.txt', 'r')\n",
    "# for line in f.readlines():\n",
    "#   print(line)/StereoSet.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python StereoSet/code/evaluation.py --gold-file $DEV_DATA_PATH --predictions-file $PREDICTION_SCORES_PATH\n",
    "!cp \"results.json\" $RESULTS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"She was confident in herself, but afraid to face the boys club in the industry. She started\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lz1u2FdkNgGp"
   },
   "outputs": [],
   "source": [
    "#??StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nZFJKRzOBoyv",
    "outputId": "14a806e9-6d35-4204-ffd4-6ea5a8f5a5b4"
   },
   "outputs": [],
   "source": [
    "# sent = \"<sos> i know you. You are a lair. i know you. You are a lair. i know you. You are a lair\"\n",
    "# tokens = tokenizer.encode_sentence(sent)\n",
    "# hypothesis = [0]*len(tokens)\n",
    "# hypothesis[-1] = 2 # <sos> token\n",
    "# '''\n",
    "# while len(tokens)<args[\"sequence_length\"]:\n",
    "#   tokens = [0]+tokens # add initial pads\n",
    "# tokens = [-args[\"sequence_length\"]:]\n",
    "# '''\n",
    "\n",
    "# def calc():\n",
    "#     model.eval()\n",
    "#     joint_sentence_probability = []\n",
    "#     state_h, state_c = model.init_state(len(tokens))\n",
    "#     state_h = state_h.to('cuda')\n",
    "#     state_c = state_c.to('cuda')\n",
    "    \n",
    "#     x = torch.tensor([tokens]).to('cuda')\n",
    "    \n",
    "#     y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "#     y_pred = y_pred.to('cuda')\n",
    "#     state_h = state_h.to('cuda')\n",
    "#     state_c = state_c.to('cuda')\n",
    "#     #print(y_pred.size())\n",
    "#     #return\n",
    "    \n",
    "#     for i in range(1, len(tokens)):\n",
    "#       p = torch.nn.functional.softmax(y_pred[0][i-1], dim=0).cpu().detach().numpy()\n",
    "#       joint_sentence_probability.append(p[tokens[i]])\n",
    "#     score = np.sum([np.log2(i) for i in joint_sentence_probability]) \n",
    "#     score /= len(joint_sentence_probability)\n",
    "#     score = np.power(2, score)\n",
    "#     print(score)\n",
    "# calc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Db2GbcvGN4w1",
    "outputId": "5d796110-2f04-4254-cf40-aa2ffb95e812"
   },
   "outputs": [],
   "source": [
    "# sent = \"<sos> i know you. You are a lair. i know you. You are a lair. i know you. You are a lair\"\n",
    "# tokens = tokenizer.encode_sentence(sent)\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "45WuUks5HikY",
    "outputId": "52256d54-fdb3-436a-efd5-227aa3bf15a2"
   },
   "outputs": [],
   "source": [
    "# !ls Stereoset_gender_intersentece_dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "j-HzR1rvXTfd",
    "outputId": "ba7830bc-66d1-4c93-fb88-7b5e8758ec15"
   },
   "outputs": [],
   "source": [
    "# f = open('Stereoset_gender_intersentece_dev_data/sentences.txt', 'r')\n",
    "# for line in f.readlines():\n",
    "#   print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQQbKOxXXvr4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenized_sentences[43536] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for line in tokenizer.tokenized_sentences:\n",
    "#     c = 0\n",
    "#     line  = line.split()\n",
    "#     for token in line:\n",
    "#         if token.strip() == \".\":\n",
    "#             c += 1\n",
    "#     if c > 1:\n",
    "#         count += 1\n",
    "\n",
    "        \n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.tensor(124.341)\n",
    "# print(a)\n",
    "# print(float(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of RNN_LM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
